{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of synthetic data for Wisoncsin Breat Cancer data set using a Variational AutoEncoder. Tested using a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim\n",
    "\n",
    "To test a a Variational AutoEncoder (VAE) for synthesising data that can be used to train a logistic regression machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Raw data is avilable at: \n",
    "\n",
    "https://www.kaggle.com/uciml/breast-cancer-wisconsin-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic methods description\n",
    "\n",
    "* Create synthetic data by use of a Variational AutoEncoder\n",
    "\n",
    "Kingma, D.P. and  Welling, M. (2013) Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,2013.\n",
    "\n",
    "* Train logistic regression model on synthetic data and test against held-back raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code & results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Turn warnings off for notebook publication\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\"\n",
    "    Load Wisconsin Breast Cancer Data Set\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X: NumPy array of X\n",
    "    y: Numpy array of y\n",
    "    col_names: column names for X\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data and drop 'id' column\n",
    "    data = pd.read_csv('./wisconsin.csv')\n",
    "    data.drop('id', axis=1, inplace=True)\n",
    "\n",
    "    # Change 'diagnosis' column to 'malignant', and put in last column place\n",
    "    malignant = pd.DataFrame()\n",
    "    data['malignant'] = data['diagnosis'] == 'M'\n",
    "    data.drop('diagnosis', axis=1, inplace=True)\n",
    "\n",
    "    # Split data in X and y\n",
    "    X = data.drop(['malignant'], axis=1)\n",
    "    y = data['malignant']\n",
    "    \n",
    "    # Get col names and convert to NumPy arrays\n",
    "    X_col_names = list(X)\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    \n",
    "    return data, X, y, X_col_names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split X and y into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_train_test(X, y, test_proportion=0.25):    \n",
    "    \"\"\"\"\n",
    "    Randomly split X and y numpy arrays into training and test data sets\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    X and y NumPy arrays\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_test, X_train, y_test, y_train Numpy arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, shuffle=True, test_size=test_proportion)\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_data(X_train, X_test):\n",
    "    \"\"\"\"\n",
    "    Standardise training and tets data sets according to mean and standard\n",
    "    deviation of test set\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    X_train, X_test NumPy arrays\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_train_std, X_test_std\n",
    "    \"\"\"\n",
    "    \n",
    "    mu = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0)\n",
    "    \n",
    "    X_train_std = (X_train - mu) / std\n",
    "    X_test_std = (X_test - mu) /std\n",
    "    \n",
    "    return X_train_std, X_test_std\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diagnostic_performance(actual, predicted):\n",
    "    \"\"\" Calculate sensitivty and specificty.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    actual, predted numpy arrays (1 = +ve, 0 = -ve)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary of results:\n",
    "    \n",
    "    1)  accuracy: proportion of test results that are correct    \n",
    "    2)  sensitivity: proportion of true +ve identified\n",
    "    3)  specificity: proportion of true -ve identified\n",
    "    4)  positive likelihood: increased probability of true +ve if test +ve\n",
    "    5)  negative likelihood: reduced probability of true +ve if test -ve\n",
    "    6)  false positive rate: proportion of false +ves in true -ve patients\n",
    "    7)  false negative rate:  proportion of false -ves in true +ve patients\n",
    "    8)  positive predictive value: chance of true +ve if test +ve\n",
    "    9)  negative predictive value: chance of true -ve if test -ve\n",
    "    10) actual positive rate: proportion of actual values that are +ve\n",
    "    11) predicted positive rate: proportion of predicted vales that are +ve\n",
    "    12) recall: same as sensitivity\n",
    "    13) precision: the proportion of predicted +ve that are true +ve\n",
    "    14) f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "    *false positive rate is the percentage of healthy individuals who \n",
    "    incorrectly receive a positive test result\n",
    "    * alse neagtive rate is the percentage of diseased individuals who \n",
    "    incorrectly receive a negative test result\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate results \n",
    "    actual_positives = actual == 1\n",
    "    actual_negatives = actual == 0\n",
    "    test_positives = predicted == 1\n",
    "    test_negatives = predicted == 0\n",
    "    test_correct = actual == predicted\n",
    "    accuracy = test_correct.mean()\n",
    "    true_positives = actual_positives & test_positives\n",
    "    false_positives = actual_negatives & test_positives\n",
    "    true_negatives = actual_negatives & test_negatives\n",
    "    sensitivity = true_positives.sum() / actual_positives.sum()\n",
    "    specificity = np.sum(true_negatives) / np.sum(actual_negatives)\n",
    "    positive_likelihood = sensitivity / (1 - specificity)\n",
    "    negative_likelihood = (1 - sensitivity) / specificity\n",
    "    false_postive_rate = 1 - specificity\n",
    "    false_negative_rate = 1 - sensitivity\n",
    "    positive_predictive_value = true_positives.sum() / test_positives.sum()\n",
    "    negative_predicitive_value = true_negatives.sum() / test_negatives.sum()\n",
    "    actual_positive_rate = actual.mean()\n",
    "    predicted_positive_rate = predicted.mean()\n",
    "    recall = sensitivity\n",
    "    precision = \\\n",
    "        true_positives.sum() / (true_positives.sum() + false_positives.sum())\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    # Add results to dictionary\n",
    "    results = dict()\n",
    "    results['accuracy'] = accuracy\n",
    "    results['sensitivity'] = sensitivity\n",
    "    results['specificity'] = specificity\n",
    "    results['positive_likelihood'] = positive_likelihood\n",
    "    results['negative_likelihood'] = negative_likelihood\n",
    "    results['false_postive_rate'] = false_postive_rate\n",
    "    results['false_postive_rate'] = false_postive_rate\n",
    "    results['false_negative_rate'] = false_negative_rate\n",
    "    results['positive_predictive_value'] = positive_predictive_value\n",
    "    results['negative_predicitive_value'] = negative_predicitive_value\n",
    "    results['actual_positive_rate'] = actual_positive_rate\n",
    "    results['predicted_positive_rate'] = predicted_positive_rate\n",
    "    results['recall'] = recall\n",
    "    results['precision'] = precision\n",
    "    results['f1'] = f1\n",
    "   \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_test_logistic_regression_model(X_train, X_test, y_train, y_test):    \n",
    "    \"\"\"\"\n",
    "    Fit and test logistic regression model. \n",
    "    Return a dictionary of accuracy measures.\n",
    "    Calls on `calculate_diagnostic_performance` to calculate results\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    X_train, X_test NumPy arrays\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary of accuracy results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fit logistic regression model \n",
    "    lr = LogisticRegression(C=0.1)\n",
    "    lr.fit(X_train,y_train)\n",
    "\n",
    "    # Predict tets set labels\n",
    "    y_pred = lr.predict(X_test_std)\n",
    "    \n",
    "    # Get accuracy results\n",
    "    accuracy_results = calculate_diagnostic_performance(y_test, y_pred)\n",
    "    \n",
    "    return accuracy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data Method - Variational AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"\n",
    "    Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    Instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "    z = z_mean + sqrt(var) * epsilon\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    import tensorflow\n",
    "    from tensorflow.keras import backend as K\n",
    "        \n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    \n",
    "    sample = z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic_data_vae(X_original, y_original, \n",
    "                            batch_size=256,\n",
    "                            latent_dim=8,\n",
    "                            epochs=10000,\n",
    "                            learning_rate=2e-5,\n",
    "                            dropout=0.25,\n",
    "                            number_of_samples=1000):\n",
    "    \"\"\"\n",
    "    Synthetic data generation.\n",
    "    Calls on `get_principal_component_model` for PCA model\n",
    "    If number of components not defined then the function sets it to the number\n",
    "      of features in X\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    original_data: X, y numpy arrays\n",
    "    number_of_samples: number of synthetic samples to generate\n",
    "    n_components: number of principal components to use for data synthesis\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_synthetic: NumPy array\n",
    "    y_synthetic: NumPy array\n",
    "\n",
    "    \"\"\"\n",
    "    import tensorflow\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras import backend as K\n",
    "    from tensorflow.keras.losses import mean_squared_error\n",
    "    \n",
    "    # Standardise X\n",
    "    mean = X_original.mean(axis=0)\n",
    "    std = X_original.mean(axis=0)    \n",
    "    X_std = (X_original - mean) / std\n",
    "    \n",
    "    # network parameters\n",
    "    input_shape = X_original.shape[1]\n",
    "    intermediate_dim = X_original.shape[1]\n",
    "   \n",
    "    # Split the training data into positive and negative\n",
    "    mask = y_original == 1\n",
    "    X_train_pos = X_std[mask]\n",
    "    mask = y_original == 0\n",
    "    X_train_neg = X_std[mask]\n",
    "    \n",
    "    # Set up list for positive and negative synthetic data sets\n",
    "    synthetic_X_sets = []\n",
    "    \n",
    "    # Run fir twice: once for positive label examples, the other for negative\n",
    "    for training_set in [X_train_pos, X_train_neg]:\n",
    "        \n",
    "        # Clear Tensorflow\n",
    "        K.clear_session()\n",
    "\n",
    "        # VAE model = encoder + decoder\n",
    "        # build encoder model\n",
    "        inputs = layers.Input(shape=input_shape, name='encoder_input')\n",
    "        \n",
    "        encode_dense_1 = layers.Dense(\n",
    "            intermediate_dim, activation='relu')(inputs)\n",
    "        \n",
    "        dropout_encoder_layer_1 = layers.Dropout(dropout)(encode_dense_1)\n",
    "        \n",
    "        encode_dense_2 = layers.Dense(\n",
    "            intermediate_dim, activation='relu')(dropout_encoder_layer_1)\n",
    "        \n",
    "        z_mean = layers.Dense(latent_dim, name='z_mean')(encode_dense_2)\n",
    "        \n",
    "        z_log_var = layers.Dense(latent_dim, name='z_log_var')(encode_dense_2)\n",
    "    \n",
    "        # use reparameterization trick to push the sampling out as input\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        z = layers.Lambda(\n",
    "            sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "    \n",
    "        # instantiate encoder model\n",
    "        encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "        # build decoder model\n",
    "        latent_inputs = layers.Input(shape=(latent_dim,), name='z_sampling')\n",
    "        \n",
    "        decode_dense_1 = layers.Dense(\n",
    "            intermediate_dim, activation='relu')(latent_inputs)\n",
    "        \n",
    "        dropout_decoder_layer_1 = layers.Dropout(dropout)(decode_dense_1)\n",
    "        \n",
    "        decode_dense_2 = layers.Dense(\n",
    "            intermediate_dim, activation='relu')(dropout_decoder_layer_1)\n",
    "        \n",
    "        outputs = layers.Dense(input_shape)(decode_dense_2)\n",
    "    \n",
    "        # instantiate decoder model\n",
    "        decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    \n",
    "        # instantiate VAE model\n",
    "        outputs = decoder(encoder(inputs)[2])\n",
    "        vae = Model(inputs, outputs, name='vae_mlp')\n",
    "    \n",
    "        # Train the autoencoder\n",
    "        \n",
    "        optimizer = Adam(lr=learning_rate)\n",
    "        \n",
    "        # VAE loss = mse_loss or xent_loss + kl_loss\n",
    "        vae.compile(optimizer=optimizer, loss = mean_squared_error)\n",
    "        \n",
    "        # Train the autoencoder\n",
    "        vae.fit(training_set, training_set,\n",
    "                batch_size = batch_size,\n",
    "                shuffle = True,\n",
    "                epochs = epochs,\n",
    "                verbose=0)\n",
    "        \n",
    "        # Produce synthetic data\n",
    "        z_new = np.random.normal(size = (number_of_samples, latent_dim))\n",
    "        reconst = decoder.predict(np.array(z_new))\n",
    "        reconst = mean + (reconst * std)\n",
    "        synthetic_X_sets.append(reconst)\n",
    "        \n",
    "        # Clear models\n",
    "        K.clear_session()\n",
    "        del encoder\n",
    "        del decoder\n",
    "        del vae\n",
    "        \n",
    "    # Combine data\n",
    "    # Combine positive and negative and shuffle rows\n",
    "    X_synthetic = np.concatenate(\n",
    "            (synthetic_X_sets[0], synthetic_X_sets[1]), axis=0)\n",
    "    \n",
    "    y_synthetic_pos = np.ones((number_of_samples, 1))\n",
    "    y_synthetic_neg = np.zeros((number_of_samples, 1))\n",
    "    \n",
    "    y_synthetic = np.concatenate((y_synthetic_pos, y_synthetic_neg), axis=0)\n",
    "    \n",
    "    # Randomise order of X, y\n",
    "    synthetic = np.concatenate((X_synthetic, y_synthetic), axis=1)\n",
    "    shuffle_index = np.random.permutation(np.arange(X_synthetic.shape[0]))\n",
    "    synthetic = synthetic[shuffle_index]\n",
    "    X_synthetic = synthetic[:,0:-1]\n",
    "    y_synthetic = synthetic[:,-1]\n",
    "                                                                   \n",
    "    return X_synthetic, y_synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "original_data, X, y, X_col_names = load_data()\n",
    "\n",
    "# Set up results DataFrame\n",
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting classification model to raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 "
     ]
    }
   ],
   "source": [
    "# Set number of replicate runs\n",
    "number_of_runs = 30\n",
    "\n",
    "# Set up lists for results\n",
    "accuracy_measure_names = []\n",
    "accuracy_measure_data = []\n",
    "\n",
    "for run in range(number_of_runs):\n",
    "    \n",
    "    # Print progress\n",
    "    print (run + 1, end=' ')\n",
    "    \n",
    "    # Split training and test set\n",
    "    X_train, X_test, y_train, y_test = split_into_train_test(X, y)\n",
    "\n",
    "    # Standardise data    \n",
    "    X_train_std, X_test_std = standardise_data(X_train, X_test)\n",
    "\n",
    "    # Get accuracy of fitted model\n",
    "    accuracy = fit_and_test_logistic_regression_model(\n",
    "        X_train_std, X_test_std, y_train, y_test)\n",
    "    \n",
    "    # Get accuracy measure names if not previously done\n",
    "    if len(accuracy_measure_names) == 0:\n",
    "        for key, value in accuracy.items():\n",
    "            accuracy_measure_names.append(key)\n",
    "    \n",
    "    # Get accuracy values\n",
    "    run_accuracy_results = []\n",
    "    for key, value in accuracy.items():\n",
    "            run_accuracy_results.append(value)\n",
    "            \n",
    "    # Add results to results list\n",
    "    accuracy_measure_data.append(run_accuracy_results)\n",
    "\n",
    "# Strore mean and sem in results DataFrame \n",
    "accuracy_array = np.array(accuracy_measure_data)\n",
    "results['raw_mean'] = accuracy_array.mean(axis=0)\n",
    "results['raw_sem'] = accuracy_array.std(axis=0)/np.sqrt(number_of_runs)\n",
    "results.index = accuracy_measure_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting classification model to synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1001 13:28:17.824539 140211181631296 deprecation.py:506] From /home/michael/anaconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3 "
     ]
    }
   ],
   "source": [
    "# Set number of replicate runs\n",
    "number_of_runs = 3\n",
    "\n",
    "for run in range(number_of_runs):\n",
    "    \n",
    "    # Print progress\n",
    "    print (run + 1, end=' ')\n",
    "\n",
    "    X_synthetic, y_synthetic = \\\n",
    "        make_synthetic_data_vae(X, y)\n",
    "\n",
    "    # Split training and test set\n",
    "    X_train, X_test, y_train, y_test = split_into_train_test(X, y)\n",
    "\n",
    "    # Standardise data (using synthetic data)\n",
    "    X_train_std, X_test_std = standardise_data(X_synthetic, X_test)\n",
    "\n",
    "    # Get accuracy of fitted model\n",
    "    accuracy = fit_and_test_logistic_regression_model(\n",
    "        X_train_std, X_test_std, y_synthetic, y_test)\n",
    "\n",
    "    # Get accuracy measure names if not previously done\n",
    "    if len(accuracy_measure_names) == 0:\n",
    "        for key, value in accuracy.items():\n",
    "            accuracy_measure_names.append(key)\n",
    "\n",
    "     # Get accuracy values\n",
    "    run_accuracy_results = []\n",
    "    for key, value in accuracy.items():\n",
    "            run_accuracy_results.append(value)\n",
    "            \n",
    "    # Add results to results list\n",
    "    accuracy_measure_data.append(run_accuracy_results)\n",
    "\n",
    "# Strore mean and sem in results DataFrame \n",
    "accuracy_array = np.array(accuracy_measure_data)\n",
    "results['vae_mean'] = accuracy_array.mean(axis=0)\n",
    "results['vae_sem'] = accuracy_array.std(axis=0)/np.sqrt(number_of_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save last synthetic data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame with id\n",
    "synth_df = pd.DataFrame()\n",
    "synth_df['id'] = np.arange(y_synthetic.shape[0])\n",
    "\n",
    "\n",
    "# Transfer X values to DataFrame\n",
    "synth_df=pd.concat([synth_df, \n",
    "                    pd.DataFrame(X_synthetic, columns=X_col_names)],\n",
    "                    axis=1)\n",
    "\n",
    "# Add a 'M' or 'B' diagnosis\n",
    "y_list = list(y_synthetic)\n",
    "diagnosis = ['M' if y==1 else 'B' for y in y_list]\n",
    "synth_df['diagnosis'] = diagnosis\n",
    "\n",
    "# Shuffle data\n",
    "synth_df = synth_df.sample(frac=1.0)\n",
    "\n",
    "# Save data\n",
    "synth_df.to_csv('./Output/synthetic_data_vae.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_mean</th>\n",
       "      <th>raw_sem</th>\n",
       "      <th>vae_mean</th>\n",
       "      <th>vae_sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.973193</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.006668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sensitivity</td>\n",
       "      <td>0.940746</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.941020</td>\n",
       "      <td>0.013246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>specificity</td>\n",
       "      <td>0.991845</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.989912</td>\n",
       "      <td>0.006829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>positive_likelihood</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>negative_likelihood</td>\n",
       "      <td>0.059703</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>0.059582</td>\n",
       "      <td>0.013381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>false_postive_rate</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.010088</td>\n",
       "      <td>0.006829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>false_negative_rate</td>\n",
       "      <td>0.059254</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>0.013246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>positive_predictive_value</td>\n",
       "      <td>0.985702</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.982368</td>\n",
       "      <td>0.012198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>negative_predicitive_value</td>\n",
       "      <td>0.966668</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>0.966663</td>\n",
       "      <td>0.007966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual_positive_rate</td>\n",
       "      <td>0.365734</td>\n",
       "      <td>0.005898</td>\n",
       "      <td>0.366603</td>\n",
       "      <td>0.018193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>predicted_positive_rate</td>\n",
       "      <td>0.349184</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.351346</td>\n",
       "      <td>0.018675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>recall</td>\n",
       "      <td>0.940746</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.941020</td>\n",
       "      <td>0.013246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>precision</td>\n",
       "      <td>0.985702</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.982368</td>\n",
       "      <td>0.012198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>f1</td>\n",
       "      <td>0.962462</td>\n",
       "      <td>0.002327</td>\n",
       "      <td>0.960989</td>\n",
       "      <td>0.009035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            raw_mean   raw_sem  vae_mean   vae_sem\n",
       "accuracy                    0.973193  0.001747  0.972028  0.006668\n",
       "sensitivity                 0.940746  0.004105  0.941020  0.013246\n",
       "specificity                 0.991845  0.001641  0.989912  0.006829\n",
       "positive_likelihood              inf       NaN       inf       NaN\n",
       "negative_likelihood         0.059703  0.004121  0.059582  0.013381\n",
       "false_postive_rate          0.008155  0.001641  0.010088  0.006829\n",
       "false_negative_rate         0.059254  0.004105  0.058980  0.013246\n",
       "positive_predictive_value   0.985702  0.002902  0.982368  0.012198\n",
       "negative_predicitive_value  0.966668  0.002482  0.966663  0.007966\n",
       "actual_positive_rate        0.365734  0.005898  0.366603  0.018193\n",
       "predicted_positive_rate     0.349184  0.006015  0.351346  0.018675\n",
       "recall                      0.940746  0.004105  0.941020  0.013246\n",
       "precision                   0.985702  0.002902  0.982368  0.012198\n",
       "f1                          0.962462  0.002327  0.960989  0.009035"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare raw and synthetic data means and standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process synthetic data\n",
    "synth_df.drop('id', axis=1, inplace=True)\n",
    "malignant = pd.DataFrame()\n",
    "synth_df['malignant'] = synth_df['diagnosis'] == 'M'\n",
    "synth_df.drop('diagnosis', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original M mean</th>\n",
       "      <th>Synthetic M mean</th>\n",
       "      <th>Original B mean</th>\n",
       "      <th>Synthetic B mean</th>\n",
       "      <th>Original M std</th>\n",
       "      <th>Synthetic M std</th>\n",
       "      <th>Original B std</th>\n",
       "      <th>Synthetic B std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>radius_mean</td>\n",
       "      <td>17.462830</td>\n",
       "      <td>16.009031</td>\n",
       "      <td>12.146524</td>\n",
       "      <td>13.056340</td>\n",
       "      <td>3.203971</td>\n",
       "      <td>1.506492</td>\n",
       "      <td>1.780512</td>\n",
       "      <td>1.124188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>texture_mean</td>\n",
       "      <td>21.604906</td>\n",
       "      <td>21.368504</td>\n",
       "      <td>17.914762</td>\n",
       "      <td>18.486146</td>\n",
       "      <td>3.779470</td>\n",
       "      <td>1.456265</td>\n",
       "      <td>3.995125</td>\n",
       "      <td>1.096615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>perimeter_mean</td>\n",
       "      <td>115.365377</td>\n",
       "      <td>105.702590</td>\n",
       "      <td>78.075406</td>\n",
       "      <td>84.283148</td>\n",
       "      <td>21.854653</td>\n",
       "      <td>8.696426</td>\n",
       "      <td>11.807438</td>\n",
       "      <td>4.139838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>area_mean</td>\n",
       "      <td>978.376415</td>\n",
       "      <td>873.929603</td>\n",
       "      <td>462.790196</td>\n",
       "      <td>563.999887</td>\n",
       "      <td>367.937978</td>\n",
       "      <td>115.848858</td>\n",
       "      <td>134.287118</td>\n",
       "      <td>71.740018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>smoothness_mean</td>\n",
       "      <td>0.102898</td>\n",
       "      <td>0.097986</td>\n",
       "      <td>0.092478</td>\n",
       "      <td>0.095174</td>\n",
       "      <td>0.012608</td>\n",
       "      <td>0.006460</td>\n",
       "      <td>0.013446</td>\n",
       "      <td>0.004776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>compactness_mean</td>\n",
       "      <td>0.145188</td>\n",
       "      <td>0.108176</td>\n",
       "      <td>0.080085</td>\n",
       "      <td>0.101601</td>\n",
       "      <td>0.053987</td>\n",
       "      <td>0.018522</td>\n",
       "      <td>0.033750</td>\n",
       "      <td>0.010665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>concavity_mean</td>\n",
       "      <td>0.160775</td>\n",
       "      <td>0.111513</td>\n",
       "      <td>0.046058</td>\n",
       "      <td>0.073543</td>\n",
       "      <td>0.075019</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.043442</td>\n",
       "      <td>0.015841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>concave points_mean</td>\n",
       "      <td>0.087990</td>\n",
       "      <td>0.064413</td>\n",
       "      <td>0.025717</td>\n",
       "      <td>0.041479</td>\n",
       "      <td>0.034374</td>\n",
       "      <td>0.012806</td>\n",
       "      <td>0.015909</td>\n",
       "      <td>0.005291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>symmetry_mean</td>\n",
       "      <td>0.192909</td>\n",
       "      <td>0.180205</td>\n",
       "      <td>0.174186</td>\n",
       "      <td>0.177848</td>\n",
       "      <td>0.027638</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>0.024807</td>\n",
       "      <td>0.007545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fractal_dimension_mean</td>\n",
       "      <td>0.062680</td>\n",
       "      <td>0.059831</td>\n",
       "      <td>0.062867</td>\n",
       "      <td>0.064192</td>\n",
       "      <td>0.007573</td>\n",
       "      <td>0.005504</td>\n",
       "      <td>0.006747</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>radius_se</td>\n",
       "      <td>0.609083</td>\n",
       "      <td>0.452296</td>\n",
       "      <td>0.284082</td>\n",
       "      <td>0.340215</td>\n",
       "      <td>0.345039</td>\n",
       "      <td>0.079924</td>\n",
       "      <td>0.112570</td>\n",
       "      <td>0.049010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>texture_se</td>\n",
       "      <td>1.210915</td>\n",
       "      <td>1.111688</td>\n",
       "      <td>1.220380</td>\n",
       "      <td>1.265597</td>\n",
       "      <td>0.483178</td>\n",
       "      <td>0.090707</td>\n",
       "      <td>0.589180</td>\n",
       "      <td>0.214174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>perimeter_se</td>\n",
       "      <td>4.323929</td>\n",
       "      <td>3.112943</td>\n",
       "      <td>2.000321</td>\n",
       "      <td>2.539347</td>\n",
       "      <td>2.568546</td>\n",
       "      <td>0.595644</td>\n",
       "      <td>0.771169</td>\n",
       "      <td>0.293514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>area_se</td>\n",
       "      <td>72.672406</td>\n",
       "      <td>48.334505</td>\n",
       "      <td>21.135148</td>\n",
       "      <td>26.662344</td>\n",
       "      <td>61.355268</td>\n",
       "      <td>14.569570</td>\n",
       "      <td>8.843472</td>\n",
       "      <td>3.714743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>smoothness_se</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>0.006257</td>\n",
       "      <td>0.007196</td>\n",
       "      <td>0.007202</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.000786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>compactness_se</td>\n",
       "      <td>0.032281</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>0.021438</td>\n",
       "      <td>0.028714</td>\n",
       "      <td>0.018387</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.005751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>concavity_se</td>\n",
       "      <td>0.041824</td>\n",
       "      <td>0.028088</td>\n",
       "      <td>0.025997</td>\n",
       "      <td>0.034840</td>\n",
       "      <td>0.021603</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>0.032918</td>\n",
       "      <td>0.010327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>concave points_se</td>\n",
       "      <td>0.015060</td>\n",
       "      <td>0.012306</td>\n",
       "      <td>0.009858</td>\n",
       "      <td>0.012564</td>\n",
       "      <td>0.005517</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>0.001702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>symmetry_se</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.016823</td>\n",
       "      <td>0.020584</td>\n",
       "      <td>0.020337</td>\n",
       "      <td>0.010065</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.006999</td>\n",
       "      <td>0.002024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fractal_dimension_se</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.004275</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.000864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>radius_worst</td>\n",
       "      <td>21.134811</td>\n",
       "      <td>19.400888</td>\n",
       "      <td>13.379801</td>\n",
       "      <td>14.322493</td>\n",
       "      <td>4.283569</td>\n",
       "      <td>2.027307</td>\n",
       "      <td>1.981368</td>\n",
       "      <td>1.034554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>texture_worst</td>\n",
       "      <td>29.318208</td>\n",
       "      <td>28.968921</td>\n",
       "      <td>23.515070</td>\n",
       "      <td>23.328282</td>\n",
       "      <td>5.434804</td>\n",
       "      <td>1.573478</td>\n",
       "      <td>5.493955</td>\n",
       "      <td>1.458306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>perimeter_worst</td>\n",
       "      <td>141.370330</td>\n",
       "      <td>129.929828</td>\n",
       "      <td>87.005938</td>\n",
       "      <td>94.028741</td>\n",
       "      <td>29.457055</td>\n",
       "      <td>7.948392</td>\n",
       "      <td>13.527091</td>\n",
       "      <td>4.260694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>area_worst</td>\n",
       "      <td>1422.286321</td>\n",
       "      <td>1144.270019</td>\n",
       "      <td>558.899440</td>\n",
       "      <td>648.064202</td>\n",
       "      <td>597.967743</td>\n",
       "      <td>152.640922</td>\n",
       "      <td>163.601424</td>\n",
       "      <td>72.516739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>smoothness_worst</td>\n",
       "      <td>0.144845</td>\n",
       "      <td>0.139715</td>\n",
       "      <td>0.124959</td>\n",
       "      <td>0.130092</td>\n",
       "      <td>0.021870</td>\n",
       "      <td>0.010646</td>\n",
       "      <td>0.020013</td>\n",
       "      <td>0.008268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>compactness_worst</td>\n",
       "      <td>0.374824</td>\n",
       "      <td>0.281795</td>\n",
       "      <td>0.182673</td>\n",
       "      <td>0.256351</td>\n",
       "      <td>0.170372</td>\n",
       "      <td>0.059912</td>\n",
       "      <td>0.092180</td>\n",
       "      <td>0.033423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>concavity_worst</td>\n",
       "      <td>0.450606</td>\n",
       "      <td>0.329287</td>\n",
       "      <td>0.166238</td>\n",
       "      <td>0.254510</td>\n",
       "      <td>0.181507</td>\n",
       "      <td>0.052055</td>\n",
       "      <td>0.140368</td>\n",
       "      <td>0.054390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>concave points_worst</td>\n",
       "      <td>0.182237</td>\n",
       "      <td>0.149391</td>\n",
       "      <td>0.074444</td>\n",
       "      <td>0.106132</td>\n",
       "      <td>0.046308</td>\n",
       "      <td>0.014777</td>\n",
       "      <td>0.035797</td>\n",
       "      <td>0.015747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>symmetry_worst</td>\n",
       "      <td>0.323468</td>\n",
       "      <td>0.303775</td>\n",
       "      <td>0.270246</td>\n",
       "      <td>0.267198</td>\n",
       "      <td>0.074685</td>\n",
       "      <td>0.025914</td>\n",
       "      <td>0.041745</td>\n",
       "      <td>0.015638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fractal_dimension_worst</td>\n",
       "      <td>0.091530</td>\n",
       "      <td>0.082444</td>\n",
       "      <td>0.079442</td>\n",
       "      <td>0.083975</td>\n",
       "      <td>0.021553</td>\n",
       "      <td>0.007852</td>\n",
       "      <td>0.013804</td>\n",
       "      <td>0.005355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>malignant</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Original M mean  Synthetic M mean  Original B mean  \\\n",
       "radius_mean                    17.462830         16.009031        12.146524   \n",
       "texture_mean                   21.604906         21.368504        17.914762   \n",
       "perimeter_mean                115.365377        105.702590        78.075406   \n",
       "area_mean                     978.376415        873.929603       462.790196   \n",
       "smoothness_mean                 0.102898          0.097986         0.092478   \n",
       "compactness_mean                0.145188          0.108176         0.080085   \n",
       "concavity_mean                  0.160775          0.111513         0.046058   \n",
       "concave points_mean             0.087990          0.064413         0.025717   \n",
       "symmetry_mean                   0.192909          0.180205         0.174186   \n",
       "fractal_dimension_mean          0.062680          0.059831         0.062867   \n",
       "radius_se                       0.609083          0.452296         0.284082   \n",
       "texture_se                      1.210915          1.111688         1.220380   \n",
       "perimeter_se                    4.323929          3.112943         2.000321   \n",
       "area_se                        72.672406         48.334505        21.135148   \n",
       "smoothness_se                   0.006780          0.006257         0.007196   \n",
       "compactness_se                  0.032281          0.021600         0.021438   \n",
       "concavity_se                    0.041824          0.028088         0.025997   \n",
       "concave points_se               0.015060          0.012306         0.009858   \n",
       "symmetry_se                     0.020472          0.016823         0.020584   \n",
       "fractal_dimension_se            0.004062          0.003047         0.003636   \n",
       "radius_worst                   21.134811         19.400888        13.379801   \n",
       "texture_worst                  29.318208         28.968921        23.515070   \n",
       "perimeter_worst               141.370330        129.929828        87.005938   \n",
       "area_worst                   1422.286321       1144.270019       558.899440   \n",
       "smoothness_worst                0.144845          0.139715         0.124959   \n",
       "compactness_worst               0.374824          0.281795         0.182673   \n",
       "concavity_worst                 0.450606          0.329287         0.166238   \n",
       "concave points_worst            0.182237          0.149391         0.074444   \n",
       "symmetry_worst                  0.323468          0.303775         0.270246   \n",
       "fractal_dimension_worst         0.091530          0.082444         0.079442   \n",
       "malignant                       1.000000          1.000000         0.000000   \n",
       "\n",
       "                         Synthetic B mean  Original M std  Synthetic M std  \\\n",
       "radius_mean                     13.056340        3.203971         1.506492   \n",
       "texture_mean                    18.486146        3.779470         1.456265   \n",
       "perimeter_mean                  84.283148       21.854653         8.696426   \n",
       "area_mean                      563.999887      367.937978       115.848858   \n",
       "smoothness_mean                  0.095174        0.012608         0.006460   \n",
       "compactness_mean                 0.101601        0.053987         0.018522   \n",
       "concavity_mean                   0.073543        0.075019         0.022696   \n",
       "concave points_mean              0.041479        0.034374         0.012806   \n",
       "symmetry_mean                    0.177848        0.027638         0.013971   \n",
       "fractal_dimension_mean           0.064192        0.007573         0.005504   \n",
       "radius_se                        0.340215        0.345039         0.079924   \n",
       "texture_se                       1.265597        0.483178         0.090707   \n",
       "perimeter_se                     2.539347        2.568546         0.595644   \n",
       "area_se                         26.662344       61.355268        14.569570   \n",
       "smoothness_se                    0.007202        0.002890         0.000780   \n",
       "compactness_se                   0.028714        0.018387         0.004546   \n",
       "concavity_se                     0.034840        0.021603         0.006674   \n",
       "concave points_se                0.012564        0.005517         0.001267   \n",
       "symmetry_se                      0.020337        0.010065         0.001863   \n",
       "fractal_dimension_se             0.004275        0.002041         0.000481   \n",
       "radius_worst                    14.322493        4.283569         2.027307   \n",
       "texture_worst                   23.328282        5.434804         1.573478   \n",
       "perimeter_worst                 94.028741       29.457055         7.948392   \n",
       "area_worst                     648.064202      597.967743       152.640922   \n",
       "smoothness_worst                 0.130092        0.021870         0.010646   \n",
       "compactness_worst                0.256351        0.170372         0.059912   \n",
       "concavity_worst                  0.254510        0.181507         0.052055   \n",
       "concave points_worst             0.106132        0.046308         0.014777   \n",
       "symmetry_worst                   0.267198        0.074685         0.025914   \n",
       "fractal_dimension_worst          0.083975        0.021553         0.007852   \n",
       "malignant                        0.000000        0.000000         0.000000   \n",
       "\n",
       "                         Original B std  Synthetic B std  \n",
       "radius_mean                    1.780512         1.124188  \n",
       "texture_mean                   3.995125         1.096615  \n",
       "perimeter_mean                11.807438         4.139838  \n",
       "area_mean                    134.287118        71.740018  \n",
       "smoothness_mean                0.013446         0.004776  \n",
       "compactness_mean               0.033750         0.010665  \n",
       "concavity_mean                 0.043442         0.015841  \n",
       "concave points_mean            0.015909         0.005291  \n",
       "symmetry_mean                  0.024807         0.007545  \n",
       "fractal_dimension_mean         0.006747         0.002930  \n",
       "radius_se                      0.112570         0.049010  \n",
       "texture_se                     0.589180         0.214174  \n",
       "perimeter_se                   0.771169         0.293514  \n",
       "area_se                        8.843472         3.714743  \n",
       "smoothness_se                  0.003061         0.000786  \n",
       "compactness_se                 0.016352         0.005751  \n",
       "concavity_se                   0.032918         0.010327  \n",
       "concave points_se              0.005709         0.001702  \n",
       "symmetry_se                    0.006999         0.002024  \n",
       "fractal_dimension_se           0.002938         0.000864  \n",
       "radius_worst                   1.981368         1.034554  \n",
       "texture_worst                  5.493955         1.458306  \n",
       "perimeter_worst               13.527091         4.260694  \n",
       "area_worst                   163.601424        72.516739  \n",
       "smoothness_worst               0.020013         0.008268  \n",
       "compactness_worst              0.092180         0.033423  \n",
       "concavity_worst                0.140368         0.054390  \n",
       "concave points_worst           0.035797         0.015747  \n",
       "symmetry_worst                 0.041745         0.015638  \n",
       "fractal_dimension_worst        0.013804         0.005355  \n",
       "malignant                      0.000000         0.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stats = pd.DataFrame()\n",
    "\n",
    "descriptive_stats['Original M mean'] = \\\n",
    "    original_data[original_data['malignant']==True].mean()\n",
    "\n",
    "descriptive_stats['Synthetic M mean'] = \\\n",
    "    synth_df[synth_df['malignant']==True].mean()\n",
    "\n",
    "descriptive_stats['Original B mean'] = \\\n",
    "    original_data[original_data['malignant']==False].mean()\n",
    "\n",
    "descriptive_stats['Synthetic B mean'] = \\\n",
    "    synth_df[synth_df['malignant']==False].mean()\n",
    "\n",
    "descriptive_stats['Original M std'] = \\\n",
    "    original_data[original_data['malignant']==True].std()\n",
    "\n",
    "descriptive_stats['Synthetic M std'] = \\\n",
    "    synth_df[synth_df['malignant']==True].std()\n",
    "\n",
    "descriptive_stats['Original B std'] = \\\n",
    "    original_data[original_data['malignant']==False].std()\n",
    "\n",
    "descriptive_stats['Synthetic B std'] = \\\n",
    "    synth_df[synth_df['malignant']==False].std()\n",
    "\n",
    "\n",
    "descriptive_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
